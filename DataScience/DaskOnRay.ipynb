{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7579846",
   "metadata": {},
   "source": [
    "# Scheduler¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32bd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.dask import ray_dask_get, enable_dask_on_ray, disable_dask_on_ray\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Start Ray.\n",
    "# Tip: If connecting to an existing cluster, use ray.init(address=\"auto\").\n",
    "ray.init()\n",
    "\n",
    "d_arr = da.from_array(np.random.randint(0, 1000, size=(256, 256)))\n",
    "\n",
    "# The Dask scheduler submits the underlying task graph to Ray.\n",
    "d_arr.mean().compute(scheduler=ray_dask_get)\n",
    "\n",
    "# Use our Dask config helper to set the scheduler to ray_dask_get globally,\n",
    "# without having to specify it on each compute call.\n",
    "enable_dask_on_ray()\n",
    "\n",
    "df = dd.from_pandas(\n",
    "    pd.DataFrame(np.random.randint(0, 100, size=(1024, 2)), columns=[\"age\", \"grade\"]),\n",
    "    npartitions=2,\n",
    ")\n",
    "df.groupby([\"age\"]).mean().compute()\n",
    "\n",
    "disable_dask_on_ray()\n",
    "\n",
    "# The Dask config helper can be used as a context manager, limiting the scope\n",
    "# of the Dask-on-Ray scheduler to the context.\n",
    "with enable_dask_on_ray():\n",
    "    d_arr.mean().compute()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b82af7",
   "metadata": {},
   "source": [
    "# Best Practice for Large Scale workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Head node. Set `num_cpus=0` to avoid tasks are being scheduled on a head node.\n",
    "RAY_scheduler_spread_threshold=0.0 ray start --head --num-cpus=0\n",
    "\n",
    "# Worker node.\n",
    "RAY_scheduler_spread_threshold=0.0 ray start --address=[head-node-address]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435b102",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3efca332",
   "metadata": {},
   "source": [
    "# Out-of-Core Data Processing¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413631fc",
   "metadata": {},
   "source": [
    "Processing datasets larger than cluster memory is supported via Ray’s object spilling: if the in-memory object store is full, objects will be spilled to external storage (local disk by default). This feature is available but off by default in Ray 1.2, and is on by default in Ray 1.3+. Please see your Ray version’s object spilling documentation for steps to enable and/or configure object spilling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31598d1",
   "metadata": {},
   "source": [
    "# Persist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d713a7",
   "metadata": {},
   "source": [
    "Dask-on-Ray patches dask.persist() in order to match Dask Distributed’s persist semantics; namely, calling dask.persist() with a Dask-on-Ray scheduler will submit the tasks to the Ray cluster and return Ray futures inlined in the Dask collection. This is nice if you wish to compute some base collection (such as a Dask array), followed by multiple different downstream computations (such as aggregations): those downstream computations will be faster since that base collection computation was kicked off early and referenced by all downstream computations, often via shared memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d673047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "# Start Ray.\n",
    "# Tip: If connecting to an existing cluster, use ray.init(address=\"auto\").\n",
    "ray.init()\n",
    "\n",
    "# Use our Dask config helper to set the scheduler to ray_dask_get globally,\n",
    "# without having to specify it on each compute call.\n",
    "enable_dask_on_ray()\n",
    "\n",
    "d_arr = da.ones(100)\n",
    "print(dask.base.collections_to_dsk([d_arr]))\n",
    "# {('ones-c345e6f8436ff9bcd68ddf25287d27f3',\n",
    "#   0): (functools.partial(<function _broadcast_trick_inner at 0x7f27f1a71f80>,\n",
    "#   dtype=dtype('float64')), (5,))}\n",
    "\n",
    "# This submits all underlying Ray tasks to the cluster and returns\n",
    "# a Dask array with the Ray futures inlined.\n",
    "d_arr_p = d_arr.persist()\n",
    "\n",
    "# Notice that the Ray ObjectRef is inlined. The dask.ones() task has\n",
    "# been submitted to and is running on the Ray cluster.\n",
    "dask.base.collections_to_dsk([d_arr_p])\n",
    "# {('ones-c345e6f8436ff9bcd68ddf25287d27f3',\n",
    "#   0): ObjectRef(8b4e50dc1ddac855ffffffffffffffffffffffff0100000001000000)}\n",
    "\n",
    "# Future computations on this persisted Dask Array will be fast since we\n",
    "# already started computing d_arr_p in the background.\n",
    "d_arr_p.sum().compute()\n",
    "d_arr_p.min().compute()\n",
    "d_arr_p.max().compute()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8d107",
   "metadata": {},
   "source": [
    "# Annotations, Resources, and Task Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35925364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "# Start Ray.\n",
    "# Tip: If connecting to an existing cluster, use ray.init(address=\"auto\").\n",
    "ray.init()\n",
    "\n",
    "# Use our Dask config helper to set the scheduler to ray_dask_get globally,\n",
    "# without having to specify it on each compute call.\n",
    "enable_dask_on_ray()\n",
    "\n",
    "# All Ray tasks that underly the Dask operations performed in an annotation\n",
    "# context will require the indicated resources: 2 CPUs and 0.01 of the custom\n",
    "# resource.\n",
    "with dask.annotate(\n",
    "    ray_remote_args=dict(num_cpus=2, resources={\"custom_resource\": 0.01})\n",
    "):\n",
    "    d_arr = da.ones(100)\n",
    "\n",
    "# Operations on the same collection can have different annotations.\n",
    "with dask.annotate(ray_remote_args=dict(resources={\"other_custom_resource\": 0.01})):\n",
    "    d_arr = 2 * d_arr\n",
    "\n",
    "# This happens outside of the annotation context, so no resource constraints\n",
    "# will be attached to the underlying Ray tasks for the sum() operation.\n",
    "sum_ = d_arr.sum()\n",
    "\n",
    "# Compute the result, passing in a default resource request that will be\n",
    "# applied to all operations that aren't already annotated with a resource\n",
    "# request. In this case, only the sum() operation will get this default\n",
    "# resource request.\n",
    "# We also give ray_remote_args, which will be given to every Ray task that\n",
    "# Dask-on-Ray submits; note that this can also be overridden for individual\n",
    "# Dask operations via the dask.annotate API.\n",
    "# NOTE: We disable graph optimization since it can break annotations,\n",
    "# see this issue: https://github.com/dask/dask/issues/7036.\n",
    "result = sum_.compute(\n",
    "    ray_remote_args=dict(max_retries=5, resources={\"another_custom_resource\": 0.01}),\n",
    "    optimize_graph=False,\n",
    ")\n",
    "print(result)\n",
    "# 200\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a2bc7",
   "metadata": {},
   "source": [
    "# Custom optimization for Dask DataFrame shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.dask import dataframe_optimize, ray_dask_get\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Start Ray.\n",
    "# Tip: If connecting to an existing cluster, use ray.init(address=\"auto\").\n",
    "ray.init()\n",
    "\n",
    "# Set the Dask DataFrame optimizer to\n",
    "# our custom optimization function, this time using the config setter as a\n",
    "# context manager.\n",
    "with dask.config.set(scheduler=ray_dask_get, dataframe_optimize=dataframe_optimize):\n",
    "    npartitions = 100\n",
    "    df = dd.from_pandas(\n",
    "        pd.DataFrame(\n",
    "            np.random.randint(0, 100, size=(10000, 2)), columns=[\"age\", \"grade\"]\n",
    "        ),\n",
    "        npartitions=npartitions,\n",
    "    )\n",
    "    # We set max_branch to infinity in order to ensure that the task-based\n",
    "    # shuffle happens in a single stage, which is required in order for our\n",
    "    # optimization to work.\n",
    "    df.set_index([\"age\"], shuffle=\"tasks\", max_branch=float(\"inf\")).head(\n",
    "        10, npartitions=-1\n",
    "    )\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a473e5",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "Dask’s custom callback abstraction is extended with Ray-specific callbacks, allowing the user to hook into the Ray task submission and execution lifecycles. With these hooks, implementing Dask-level scheduler and task introspection, such as progress reporting, diagnostics, caching, etc., is simple.\n",
    "\n",
    "Here’s an example that measures and logs the execution time of each task using the ray_pretask and ray_posttask hooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.util.dask import RayDaskCallback\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "class MyTimerCallback(RayDaskCallback):\n",
    "   def _ray_pretask(self, key, object_refs):\n",
    "      # Executed at the start of the Ray task.\n",
    "      start_time = timer()\n",
    "      return start_time\n",
    "\n",
    "   def _ray_posttask(self, key, result, pre_state):\n",
    "      # Executed at the end of the Ray task.\n",
    "      execution_time = timer() - pre_state\n",
    "      print(f\"Execution time for task {key}: {execution_time}s\")\n",
    "\n",
    "\n",
    "with MyTimerCallback():\n",
    "   # Any .compute() calls within this context will get MyTimerCallback()\n",
    "   # as a Dask-Ray callback.\n",
    "   z.compute(scheduler=ray_dask_get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7442474",
   "metadata": {},
   "source": [
    "See the docstring for RayDaskCallback.__init__() <ray.util.dask.callbacks.RayDaskCallback>.__init__() for further details about these callbacks, their arguments, and their return values.\n",
    "\n",
    "When creating your own callbacks, you can use RayDaskCallback directly, passing the callback functions as constructor arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd724dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_presubmit_cb(task, key, deps):\n",
    "   print(f\"About to submit task {key}!\")\n",
    "\n",
    "with RayDaskCallback(ray_presubmit=my_presubmit_cb):\n",
    "   z.compute(scheduler=ray_dask_get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aab25e",
   "metadata": {},
   "source": [
    "or you can subclass it, implementing the callback methods that you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPresubmitCallback(RayDaskCallback):\n",
    "   def _ray_presubmit(self, task, key, deps):\n",
    "      print(f\"About to submit task {key}!\")\n",
    "\n",
    "with MyPresubmitCallback():\n",
    "   z.compute(scheduler=ray_dask_get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcacc119",
   "metadata": {},
   "source": [
    "You can also specify multiple callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa973fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hooks for both MyTimerCallback and MyPresubmitCallback will be\n",
    "# called.\n",
    "with MyTimerCallback(), MyPresubmitCallback():\n",
    "   z.compute(scheduler=ray_dask_get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3169a194",
   "metadata": {},
   "source": [
    "Combining Dask callbacks with an actor yields simple patterns for stateful data aggregation, such as capturing task execution statistics and caching results. Here is an example that does both, caching the result of a task if its execution time exceeds some user-defined threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class SimpleCacheActor:\n",
    "   def __init__(self):\n",
    "      self.cache = {}\n",
    "\n",
    "   def get(self, key):\n",
    "      # Raises KeyError if key isn't in cache.\n",
    "      return self.cache[key]\n",
    "\n",
    "   def put(self, key, value):\n",
    "      self.cache[key] = value\n",
    "\n",
    "\n",
    "class SimpleCacheCallback(RayDaskCallback):\n",
    "   def __init__(self, cache_actor_handle, put_threshold=10):\n",
    "      self.cache_actor = cache_actor_handle\n",
    "      self.put_threshold = put_threshold\n",
    "\n",
    "   def _ray_presubmit(self, task, key, deps):\n",
    "      try:\n",
    "         return ray.get(self.cache_actor.get.remote(str(key)))\n",
    "      except KeyError:\n",
    "         return None\n",
    "\n",
    "   def _ray_pretask(self, key, object_refs):\n",
    "      start_time = timer()\n",
    "      return start_time\n",
    "\n",
    "   def _ray_posttask(self, key, result, pre_state):\n",
    "      execution_time = timer() - pre_state\n",
    "      if execution_time > self.put_threshold:\n",
    "         self.cache_actor.put.remote(str(key), result)\n",
    "\n",
    "\n",
    "cache_actor = SimpleCacheActor.remote()\n",
    "cache_callback = SimpleCacheCallback(cache_actor, put_threshold=2)\n",
    "with cache_callback:\n",
    "   z.compute(scheduler=ray_dask_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4a377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533380a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ds_study')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "579d596734065d05666b929f0790d712354e78549b96b8d5da4e08003c131772"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
